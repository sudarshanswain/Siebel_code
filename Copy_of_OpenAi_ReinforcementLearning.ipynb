{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of OpenAi_ReinforcementLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudarshanswain/Siebel_code/blob/master/Copy_of_OpenAi_ReinforcementLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzHw0DtxaUhy",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"http://www.it.uu.se/research/systems_and_control/education/2017/relearn/RL.png\" alt=\"drawing\" width=\"1080\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZDG4j8wadJX",
        "colab_type": "text"
      },
      "source": [
        "**Getting familiar to Reinforcement Learning**\n",
        "- [OpenAI Spinning up Workshop](https://www.youtube.com/watch?v=fdY7dt3ijgY)\n",
        "- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AplUS-wpa8sZ",
        "colab_type": "text"
      },
      "source": [
        "**Key points from OpenAI Spinning up Workshop**\n",
        "\n",
        "- Deep RL will be key technology in AGI([Artificial General Intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence))\n",
        "\n",
        "\n",
        "What is Deep RL?\n",
        "- DeepLearning + ReinforcementLearning.\n",
        "- DL  :  **Solving problems with DNN(Deep Neural Network)**\n",
        "- RL  :  **Solving problems by Trail and Error**\n",
        "- Deep RL  :  **Trains Deep Neural Networks with Trail and Errors.**\n",
        "\n",
        "When to use RL?\n",
        "- when we dont know what to do for a particular situation.\n",
        "- for Sequencial decision-making problem. ( Driving a car and controlling all the parameters there in the car.eg: when to turn left or right, when to stop, etc ...\n",
        "- for instance for an observation we have lots of data for that we can use supervised Learning. but for some instance where gatthering Data is Difficult then we cuse RL.(Decision Making)\n",
        "- RL is useful when evaluating a behaviors is easier than generating them or solve them.\n",
        "\n",
        "when to use DL?\n",
        "- To Approximating complicated function using Data Points.\n",
        "- a lot of Data is available.\n",
        "- High Dimentional Data.\n",
        "- Examples\n",
        " - Image Recognation, Image Classification, SentimentAnalysis, Machine Translation, SpeechRecognation, Object Detection.\n",
        "\n",
        "\n",
        "When to use Deep RL?\n",
        "- Scence based decision ( Play a video game the raw pixels of Image)\n",
        "\n",
        "![](https://miro.medium.com/max/617/0*7PvoCPJuUAblMdcn.png)\n",
        "- Controling bots in simulation\n",
        "\n",
        "![](https://miro.medium.com/proxy/1*oMSg2_mKguAGKy1C64UFlw.gif)\n",
        "- Play Go, Dota, AlphaGo, Checkers and many more( Superhuman levels game play)\n",
        "\n",
        "![](https://qbi.uq.edu.au/files/27603/AlphaGo-Google-AI_QBI-700.jpg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fxmXaqJhMqU",
        "colab_type": "text"
      },
      "source": [
        "# Recap of Deep Learning Patterns\n",
        "\n",
        "**High Level Understanding of Deep Learning:**\n",
        "\n",
        "1. Find a model to get the right outputs for certain inputs\n",
        "2. Representing a model as a function of parameters   [ Y = f(x) ].\n",
        "3. evaluation of model performance with a differentiable Loss function which depends on data\n",
        "4. we can get the Optimal model by Gradient Descent on the loss.\n",
        "\n",
        "\n",
        "- Some time gradient decent will removes the losses where the model overfits the model and the model will be more specific to the trained data which does not work phenonimally on unseen data then \n",
        "- Regulizers come into Action and makes optimizers better served.\n",
        "- we trade of the loss where it will not effect the model but we will add some noice to make the model realizes the it is ok to not be  100% accurate ( which means dont overfit the data).\n",
        "- Normalization makes otimizers easier\n",
        "- Adam makes optimzers faster.\n",
        "- Reparametarization trick \n",
        "\n",
        "and many more.....\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWviAEwwjel1",
        "colab_type": "text"
      },
      "source": [
        "What makes a Deep Learning (DEEP)?\n",
        "- Deep refers to the complexcity of function.\n",
        "- function Composition refers to bunch of parametarized function for which the output of one function is the inout for others.\n",
        "- this refers to many Neural Network Architectures like LSTM and Trasformer Architectures\n",
        "- Or simple MLP Models( bl=ias and weight)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g17qv0gdroAr",
        "colab_type": "text"
      },
      "source": [
        "# Characteristics of Reinforcement Learning\n",
        "\n",
        "Here are important characteristics of reinforcement learning\n",
        "\n",
        "- There is no supervisor, only a real number or reward signal\n",
        "- Time plays a crucial role in Reinforcement problems\n",
        "- Feedback is always delayed, not instantaneous\n",
        "- Agent's actions determine the subsequent data it receives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vfv4QgKnApg",
        "colab_type": "text"
      },
      "source": [
        "# How to formulate RL problems?\n",
        "\n",
        "![](https://i2.wp.com/webindream.com/wp-content/uploads/2017/12/Reinforcement_learning_diagram.svg_.png?resize=648%2C626)\n",
        "\n",
        "Lets get familiar with RL terinologies:\n",
        "\n",
        "\n",
        "Here are some important terms used in Reinforcement AI:\n",
        "\n",
        "- Agent: It is an assumed entity which performs actions in an environment to gain some reward.\n",
        "- Environment (e): A scenario that an agent has to face.\n",
        "- State (s): State refers to the complete description of the environment\n",
        "- Value (V): It is expected long-term reward with discount, as compared to the short-term reward.\n",
        "- Value Function: It specifies the value of a state that is the total amount of reward. It is an agent which should be expected beginning from that state.\n",
        "- Model of the environment: This mimics the behavior of the environment. It helps you to make inferences to be made and also determine how the environment will behave.\n",
        "- Q value or action value (Q): Q value is quite similar to value. The only difference between the two is that it takes an additional parameter as a current action.\n",
        "- Q*: approximator\n",
        "- Q(theta): mean-sqaured Bellman error\n",
        "- Observation: This is what the agent sees about the current state of the environment.\n",
        "  - Fully Observed: agent observes the whole state\n",
        "  - Partial Observed: doesn't Observes th full state.\n",
        "- Policy (Ï€): It is a strategy which applies by the agent to decide the next action based on the current state.\n",
        "  - Stocastic: On a probability of actions, random actions are selected.\n",
        "  - deterministic: directly maps to action\n",
        "- Trajectory(T): sequence of states and actions to the history of environment. Also known as Episode or rollout.\n",
        "- Reward (R): An immediate return given to an agent when he or she performs specific action or task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2vOvtzIqyqf",
        "colab_type": "text"
      },
      "source": [
        "# Simple Example:\n",
        "\n",
        "Let's see some simple example which helps you to illustrate the reinforcement learning mechanism.\n",
        "\n",
        "Consider the scenario of teaching new tricks to your cat\n",
        "\n",
        "- As cat doesn't understand English or any other human language, we can't tell her directly what to do. Instead, we follow a different strategy.\n",
        "- We emulate a situation, and the cat tries to respond in many different ways. - If the cat's response is the desired way, we will give her fish.\n",
        "- Now whenever the cat is exposed to the same situation, the cat executes a similar action with even more enthusiastically in expectation of getting more reward(food).\n",
        "- That's like learning that cat gets from \"what to do\" from positive experiences.\n",
        "- At the same time, the cat also learns what not do when faced with negative experiences.\n",
        "\n",
        "\n",
        "In this case,\n",
        "\n",
        "- Your cat is an agent that is exposed to the environment. In this case, it is your house. An example of a state could be your cat sitting, and you use a specific word in for cat to walk.\n",
        "- Our agent reacts by performing an action transition from one \"state\" to another \"state.\"\n",
        "- For example, your cat goes from sitting to walking.\n",
        "- The reaction of an agent is an action, and the policy is a method of selecting an action given a state in expectation of better outcomes.\n",
        "- After the transition, they may get a reward or penalty in return.\n",
        "\n",
        "In general:\n",
        "- An agent Interacts with Environment\n",
        "- The goal is to MAximize the cummulative reward(called return)\n",
        "- The agent figures out how to attain its goal by trial and Error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz3jo5os5PBl",
        "colab_type": "text"
      },
      "source": [
        "# Types of Reinforcement Algorithms?\n",
        "![](https://miro.medium.com/max/1000/1*BsN4a2N1EDmgG19wWDd9CQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMS81avd6YtF",
        "colab_type": "text"
      },
      "source": [
        "All Algorithms do the Same:\n",
        "1. Run Policy\n",
        "2. Evaluate Policy\n",
        "3. Improve Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V9QZVRg6xSd",
        "colab_type": "text"
      },
      "source": [
        "# Policy Optimization:\n",
        "- \n",
        "- \n",
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFusvUEv64QP",
        "colab_type": "text"
      },
      "source": [
        "# Q-Learning\n",
        "- \n",
        "- \n",
        "- "
      ]
    }
  ]
}